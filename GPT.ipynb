{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc4248-1051-4015-9e69-2ac200280142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "========================================================================\n",
    "GPT MODELS TUTORIAL - Complete Guide with Hugging Face Transformers\n",
    "========================================================================\n",
    "\n",
    "This notebook covers:\n",
    "1. Understanding GPT Architecture\n",
    "2. Model Setup and Configuration\n",
    "3. Text Generation Techniques\n",
    "4. Creative Writing Applications\n",
    "5. Fine-tuning Basics\n",
    "6. Advanced Use Cases\n",
    "7. Best Practices and Tips\n",
    "\n",
    "Author: GPT Tutorial\n",
    "Date: 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1221b6f-d054-4422-8af4-385f06d480e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ========================================================================\n",
    "# SECTION 1: INSTALLATION AND IMPORTS\n",
    "# ========================================================================\n",
    "\n",
    "\"\"\"\n",
    "First, install required packages:\n",
    "!pip install transformers torch sentencepiece accelerate\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer,\n",
    "    GPT2Config,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPT MODELS TUTORIAL\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8e65f-351a-43e7-b5da-a26905fc3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ========================================================================\n",
    "# SECTION 2: UNDERSTANDING GPT\n",
    "# ========================================================================\n",
    "\n",
    "\"\"\"\n",
    "GPT (Generative Pre-trained Transformer) Overview:\n",
    "--------------------------------------------------\n",
    "- Architecture: Decoder-only Transformer\n",
    "- Training: Autoregressive language modeling (predict next token)\n",
    "- Strengths: Text generation, completion, creative writing\n",
    "- Variants: GPT-2, GPT-Neo, GPT-J, etc.\n",
    "\n",
    "Key Characteristics:\n",
    "- Unidirectional (left-to-right) attention\n",
    "- Generates coherent, contextually relevant text\n",
    "- Can be fine-tuned for specific tasks\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5d644-3ee1-4bc5-9386-c687fd5ae7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ========================================================================\n",
    "# SECTION 3: BASIC MODEL SETUP\n",
    "# ========================================================================\n",
    "\n",
    "class GPTBasics:\n",
    "    \"\"\"Understanding GPT model basics\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        \"\"\"\n",
    "        Initialize GPT model\n",
    "        \n",
    "        Available models:\n",
    "        - gpt2 (124M parameters)\n",
    "        - gpt2-medium (355M parameters)\n",
    "        - gpt2-large (774M parameters)\n",
    "        - gpt2-xl (1.5B parameters)\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Loading {model_name} model...\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Set padding token\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.config.pad_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"✓ Model loaded successfully\")\n",
    "        print(f\"✓ Vocabulary size: {len(self.tokenizer)}\")\n",
    "        print(f\"✓ Model parameters: {self.count_parameters():,}\")\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.model.parameters())\n",
    "    \n",
    "    def tokenization_demo(self, text):\n",
    "        \"\"\"Demonstrate tokenization process\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"TOKENIZATION DEMONSTRATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        print(f\"\\nOriginal text:\\n'{text}'\")\n",
    "        \n",
    "        # Encode\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        print(f\"\\nToken IDs: {tokens}\")\n",
    "        \n",
    "        # Decode individual tokens\n",
    "        print(\"\\nIndividual tokens:\")\n",
    "        for i, token_id in enumerate(tokens):\n",
    "            token_text = self.tokenizer.decode([token_id])\n",
    "            print(f\"  {i}: {token_id} -> '{token_text}'\")\n",
    "        \n",
    "        # Decode back to text\n",
    "        decoded = self.tokenizer.decode(tokens)\n",
    "        print(f\"\\nDecoded text:\\n'{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a64277-aa07-4911-ad88-f4a03ac9b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ========================================================================\n",
    "# SECTION 4: TEXT GENERATION TECHNIQUES\n",
    "# ========================================================================\n",
    "\n",
    "class TextGeneration:\n",
    "    \"\"\"Various text generation strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.eval()\n",
    "    \n",
    "    def greedy_search(self, prompt, max_length=50):\n",
    "        \"\"\"\n",
    "        Greedy Search: Always pick the most likely next token\n",
    "        Pros: Fast, deterministic\n",
    "        Cons: Repetitive, less creative\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"GREEDY SEARCH (Deterministic)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=False  # Greedy\n",
    "        )\n",
    "        \n",
    "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated:\\n{text}\")\n",
    "    \n",
    "    def beam_search(self, prompt, max_length=50, num_beams=5):\n",
    "        \"\"\"\n",
    "        Beam Search: Keep top K candidates at each step\n",
    "        Pros: Better quality than greedy\n",
    "        Cons: Still can be repetitive\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"BEAM SEARCH (num_beams={num_beams})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        \n",
    "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated:\\n{text}\")\n",
    "    \n",
    "    def sampling(self, prompt, max_length=50, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Sampling: Randomly sample from probability distribution\n",
    "        \n",
    "        Temperature controls randomness:\n",
    "        - temperature < 1.0: More focused, conservative\n",
    "        - temperature = 1.0: Normal sampling\n",
    "        - temperature > 1.0: More random, creative\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"SAMPLING (temperature={temperature})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated:\\n{text}\")\n",
    "    \n",
    "    def top_k_sampling(self, prompt, max_length=50, top_k=50):\n",
    "        \"\"\"\n",
    "        Top-K Sampling: Sample from top K most likely tokens\n",
    "        Reduces chance of picking unlikely words\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TOP-K SAMPLING (k={top_k})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_k=top_k,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        \n",
    "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated:\\n{text}\")\n",
    "    \n",
    "    def top_p_sampling(self, prompt, max_length=50, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Top-P (Nucleus) Sampling: Sample from smallest set \n",
    "        whose cumulative probability exceeds P\n",
    "        \n",
    "        More dynamic than top-k, adapts to distribution shape\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TOP-P (NUCLEUS) SAMPLING (p={top_p})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        \n",
    "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated:\\n{text}\")\n",
    "    \n",
    "    def combined_sampling(self, prompt, max_length=100, \n",
    "                         temperature=0.8, top_k=50, top_p=0.95):\n",
    "        \"\"\"\n",
    "        Combined Strategy: Top-K + Top-P + Temperature\n",
    "        Best practice for balanced generation\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"COMBINED SAMPLING (Recommended)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Parameters: temp={temperature}, top_k={top_k}, top_p={top_p}\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            no_repeat_ngram_size=2  # Avoid repeating 2-grams\n",
    "        )\n",
    "        \n",
    "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated:\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6c5bc-c4ae-4907-a9bb-415fde7040c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# SECTION 5: CREATIVE WRITING APPLICATIONS\n",
    "# ========================================================================\n",
    "\n",
    "class CreativeWriting:\n",
    "    \"\"\"Use GPT for creative writing tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.generator = pipeline('text-generation', model='gpt2')\n",
    "    \n",
    "    def story_writer(self, beginning, style=\"adventure\"):\n",
    "        \"\"\"Generate creative stories\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"STORY GENERATION - {style.upper()} STYLE\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        prompt = f\"{beginning}\"\n",
    "        \n",
    "        result = self.generator(\n",
    "            prompt,\n",
    "            max_length=200,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.9,\n",
    "            top_p=0.95,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{result[0]['generated_text']}\")\n",
    "    \n",
    "    def poem_generator(self, theme):\n",
    "        \"\"\"Generate poetry\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"POETRY GENERATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        prompt = f\"A poem about {theme}:\\n\\n\"\n",
    "        \n",
    "        result = self.generator(\n",
    "            prompt,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.9\n",
    "        )\n",
    "        \n",
    "        print(result[0]['generated_text'])\n",
    "    \n",
    "    def dialogue_writer(self, context):\n",
    "        \"\"\"Generate dialogue between characters\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"DIALOGUE GENERATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        result = self.generator(\n",
    "            context,\n",
    "            max_length=150,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        \n",
    "        print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0bc24e-ff8a-4ece-98d0-c93e660fde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================================================\n",
    "# SECTION 6: ADVANCED USE CASES\n",
    "# ========================================================================\n",
    "\n",
    "class AdvancedUseCases:\n",
    "    \"\"\"Advanced GPT applications\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.eval()\n",
    "    \n",
    "    def conditional_generation(self, prompt, prefix=\"\"):\n",
    "        \"\"\"Generate text with specific prefix constraint\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"CONDITIONAL GENERATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        full_prompt = f\"{prefix} {prompt}\" if prefix else prompt\n",
    "        \n",
    "        inputs = self.tokenizer.encode(full_prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=100,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        \n",
    "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Prompt: {full_prompt}\")\n",
    "        print(f\"\\nGenerated:\\n{text}\")\n",
    "    \n",
    "    def batch_generation(self, prompts):\n",
    "        \"\"\"Generate text for multiple prompts efficiently\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"BATCH GENERATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_length=80,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        \n",
    "        for i, output in enumerate(outputs):\n",
    "            text = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "            print(f\"\\nPrompt {i+1}: {prompts[i]}\")\n",
    "            print(f\"Generated: {text}\")\n",
    "    \n",
    "    def get_perplexity(self, text):\n",
    "        \"\"\"\n",
    "        Calculate perplexity (measure of uncertainty)\n",
    "        Lower perplexity = more confident/natural text\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"PERPLEXITY CALCULATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "        \n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Perplexity: {perplexity.item():.2f}\")\n",
    "        print(f\"(Lower is better - more natural text)\")\n",
    "        \n",
    "        return perplexity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114a6a6-f390-4f76-b896-a059e2444afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================================================\n",
    "# SECTION 7: BEST PRACTICES AND TIPS\n",
    "# ========================================================================\n",
    "\n",
    "\"\"\"\n",
    "BEST PRACTICES FOR GPT TEXT GENERATION\n",
    "=======================================\n",
    "\n",
    "1. PROMPT ENGINEERING\n",
    "   - Be specific and clear\n",
    "   - Provide context and examples\n",
    "   - Use formatting (bullets, numbers) when needed\n",
    "   \n",
    "2. PARAMETER TUNING\n",
    "   - temperature: 0.7-0.9 for creative tasks\n",
    "   - temperature: 0.3-0.5 for factual tasks\n",
    "   - top_p: 0.9-0.95 is generally good\n",
    "   - top_k: 40-50 works well\n",
    "   \n",
    "3. AVOIDING REPETITION\n",
    "   - Use no_repeat_ngram_size=2 or 3\n",
    "   - Lower temperature slightly\n",
    "   - Use diverse beam search\n",
    "   \n",
    "4. CONTROLLING LENGTH\n",
    "   - max_length: Set reasonable limits\n",
    "   - min_length: Ensure minimum output\n",
    "   - Use length_penalty in beam search\n",
    "   \n",
    "5. QUALITY IMPROVEMENTS\n",
    "   - Use larger models for better quality\n",
    "   - Fine-tune on domain-specific data\n",
    "   - Post-process outputs (filtering, formatting)\n",
    "   \n",
    "6. PERFORMANCE OPTIMIZATION\n",
    "   - Use batch processing for multiple prompts\n",
    "   - Cache models in production\n",
    "   - Consider quantization for inference\n",
    "   - Use GPU when available\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be0557-cb23-4a65-8628-0552e165db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GPT TUTORIAL - COMPLETE DEMONSTRATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Basics\n",
    "    print(\"\\n\\n### PART 1: BASICS ###\")\n",
    "    basics = GPTBasics(\"gpt2\")\n",
    "    basics.tokenization_demo(\"Hello, how are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6905d-7206-4137-83de-1f8c8ae5e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 2. Generation Techniques\n",
    "    print(\"\\n\\n### PART 2: GENERATION TECHNIQUES ###\")\n",
    "    generator = TextGeneration(\"gpt2\")\n",
    "    \n",
    "    prompt = \"The future of artificial intelligence\"\n",
    "    \n",
    "    generator.greedy_search(prompt, max_length=60)\n",
    "    generator.sampling(prompt, max_length=60, temperature=0.7)\n",
    "    generator.top_k_sampling(prompt, max_length=60)\n",
    "    generator.top_p_sampling(prompt, max_length=60)\n",
    "    generator.combined_sampling(prompt, max_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d275f0-1603-4999-8d88-f98261df49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 3. Creative Writing\n",
    "    print(\"\\n\\n### PART 3: CREATIVE WRITING ###\")\n",
    "    writer = CreativeWriting()\n",
    "    \n",
    "    writer.story_writer(\n",
    "        \"In a world where dreams could be recorded and shared,\",\n",
    "        style=\"sci-fi\"\n",
    "    )\n",
    "    \n",
    "    writer.dialogue_writer(\n",
    "        'Character A: \"What do you think about the new technology?\"\\n'\n",
    "        'Character B:'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650fbc6b-60ac-4fef-ac66-a60ce4fd3701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c460f4c-c167-4d37-82ae-3893e4b002d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 4. Advanced Use Cases\n",
    "    print(\"\\n\\n### PART 4: ADVANCED USE CASES ###\")\n",
    "    advanced = AdvancedUseCases(\"gpt2\")\n",
    "    \n",
    "    advanced.conditional_generation(\n",
    "        \"will transform society\",\n",
    "        prefix=\"Artificial intelligence\"\n",
    "    )\n",
    "    \n",
    "    advanced.batch_generation([\n",
    "        \"The best way to learn programming is\",\n",
    "        \"Climate change affects our planet by\",\n",
    "        \"The benefits of regular exercise include\"\n",
    "    ])\n",
    "    \n",
    "    advanced.get_perplexity(\"The quick brown fox jumps over the lazy dog.\")\n",
    "    advanced.get_perplexity(\"Colorless green ideas sleep furiously.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DEMONSTRATION COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n### KEY TAKEAWAYS ###\")\n",
    "    print(\"\"\"\n",
    "    1. GPT excels at text generation and completion\n",
    "    2. Different sampling strategies produce different outputs\n",
    "    3. Temperature controls creativity vs. coherence\n",
    "    4. Combining top-k and top-p gives best results\n",
    "    5. Proper prompting is crucial for quality output\n",
    "    6. Fine-tuning can improve domain-specific performance\n",
    "    \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26c1c4-72ed-40c3-bd5e-63566dd4ff3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770fb355-dd30-44df-8d56-f68c4d77ba1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0069d72d-c2ec-44db-aa11-e0eea6178534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120fcd7-1ddf-4e14-a021-3a2890f1a9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e73a9-9739-46a7-8b52-371ca1100635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b00b2-e62d-4898-9a61-da96f7e10172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba45361-7064-4303-8183-7048f952475e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8df969-1b69-4c96-a856-3671100dde19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
